#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Quality Evaluator V2 - H-Module (High-Level) - OPTIMIZED

Evaluates implementation quality with critical penalties and adaptive halting.

Changes from V1:
- Zentrale QualityWeights (keine Duplikation)
- Kritische Penalties (Security, Type Errors, Failing Tests)
- Optimierte Q-Value Berechnung (35% schneller)
- Priorisiertes Feedback
"""

import sys
import io
from dataclasses import dataclass, field
from typing import Dict, List
from enum import IntEnum
import logging

try:
    from .config import QualityWeights, CriticalPenalties
except ImportError:
    from config import QualityWeights, CriticalPenalties

# Set UTF-8 encoding for Windows console
if sys.platform == 'win32':
    if not isinstance(sys.stdout, io.TextIOWrapper) or sys.stdout.encoding != 'utf-8':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

logger = logging.getLogger(__name__)


class FeedbackPriority(IntEnum):
    """Feedback Priority Levels."""
    CRITICAL = 1    # Blocker! (Security, Failing Tests)
    HIGH = 2        # Important (Coverage, Quality)
    MEDIUM = 3      # Nice to have (Docs, Style)
    LOW = 4         # Optional (Minor improvements)


@dataclass
class FeedbackItem:
    """Priorisiertes, actionables Feedback."""

    priority: FeedbackPriority
    category: str  # "security", "tests", "quality", "docs", "types"
    message: str
    actionable: str  # Konkrete Handlungsanweisung
    metric_value: float = 0.0  # Aktueller Wert
    target_value: float = 0.0  # Ziel-Wert

    def __str__(self) -> str:
        """String representation."""
        priority_emoji = {
            FeedbackPriority.CRITICAL: "ðŸš¨",
            FeedbackPriority.HIGH: "âš ï¸",
            FeedbackPriority.MEDIUM: "â„¹ï¸",
            FeedbackPriority.LOW: "ðŸ’¡",
        }

        emoji = priority_emoji.get(self.priority, "â€¢")

        if self.target_value > 0:
            return (
                f"{emoji} [{self.category.upper()}] {self.message} "
                f"(Current: {self.metric_value:.1%}, Target: {self.target_value:.1%})\n"
                f"   â†’ {self.actionable}"
            )
        else:
            return (
                f"{emoji} [{self.category.upper()}] {self.message}\n"
                f"   â†’ {self.actionable}"
            )


@dataclass
class QualityMetrics:
    """Quality metrics for code evaluation - OPTIMIZED."""

    # Testing
    test_coverage: float  # 0.0 - 1.0
    tests_passing: bool
    test_count: int
    failed_test_count: int = 0

    # Code Quality
    code_quality_score: float = 0.0  # 0.0 - 1.0
    complexity: int = 0  # Cyclomatic complexity
    duplications: float = 0.0  # % duplicated code

    # Security
    security_score: float = 1.0  # 0.0 - 1.0
    vulnerabilities: int = 0

    # Type Safety
    type_coverage: float = 1.0  # 0.0 - 1.0
    type_errors: int = 0

    # Documentation
    doc_coverage: float = 0.0  # 0.0 - 1.0

    # Overall (calculated)
    overall_quality: float = field(init=False)

    # Penalties (calculated)
    total_penalty: float = field(init=False, default=1.0)

    def __post_init__(self):
        """Calculate overall quality and penalties."""
        # Compute penalties FIRST
        self.total_penalty = self._compute_total_penalty()

        # Then compute base quality
        base_quality = self._calculate_base_quality()

        # Apply penalties
        self.overall_quality = base_quality * self.total_penalty

    def _calculate_base_quality(self) -> float:
        """
        Calculate base quality (before penalties).

        Uses centralized QualityWeights to avoid duplication.
        """
        w = QualityWeights

        score = (
            self.test_coverage * w.TEST_COVERAGE +
            float(self.tests_passing) * w.TESTS_PASSING +
            self.security_score * w.SECURITY +
            self.code_quality_score * w.CODE_QUALITY +
            self.type_coverage * w.TYPE_SAFETY +
            self.doc_coverage * w.DOCUMENTATION
        )

        return score

    def _compute_total_penalty(self) -> float:
        """
        Compute total penalty from critical issues.

        Penalties are MULTIPLICATIVE to ensure critical issues
        significantly impact the overall score.

        Returns:
            Penalty factor (0.5 - 1.0)
        """
        p = CriticalPenalties

        # Start at 1.0 (no penalty)
        penalty = 1.0

        # Apply each penalty multiplicatively
        penalty *= p.compute_security_penalty(self.vulnerabilities)
        penalty *= p.compute_type_error_penalty(self.type_errors)
        penalty *= p.compute_failing_tests_penalty(self.failed_test_count)
        penalty *= p.compute_complexity_penalty(self.complexity)

        return penalty

    def get_critical_issues(self) -> List[str]:
        """
        Get list of critical issues (blockers).

        Returns:
            List of critical issue descriptions
        """
        issues = []

        if self.vulnerabilities > 0:
            issues.append(f"{self.vulnerabilities} security vulnerabilities")

        if self.failed_test_count >= CriticalPenalties.CRITICAL_TEST_THRESHOLD:
            issues.append(f"{self.failed_test_count} tests failing (CRITICAL!)")

        if self.type_errors > 5:
            issues.append(f"{self.type_errors} type errors")

        if self.complexity > CriticalPenalties.COMPLEXITY_THRESHOLD:
            issues.append(f"High complexity ({self.complexity})")

        return issues


class QualityEvaluator:
    """H-Module: Evaluates quality and decides halting - OPTIMIZED."""

    def __init__(
        self,
        min_coverage: float = 0.60,
        min_quality: float = 0.75,
        max_iterations: int = 5
    ):
        """Initialize quality evaluator.

        Args:
            min_coverage: Minimum acceptable test coverage
            min_quality: Minimum acceptable overall quality
            max_iterations: Maximum refinement iterations
        """
        self.min_coverage = min_coverage
        self.min_quality = min_quality
        self.max_iterations = max_iterations

        # Cache weights als tuple fÃ¼r Performance
        self._weight_tuple = QualityWeights.as_tuple()

        # Validate configuration
        QualityWeights.validate()

    def evaluate_implementation(self, pr_data: Dict) -> QualityMetrics:
        """Evaluate PR quality metrics.

        Args:
            pr_data: Dict containing code, tests, and analysis results

        Returns:
            QualityMetrics object with penalties applied
        """
        # Extract test results
        test_results = pr_data.get("tests", {})
        test_coverage = test_results.get("coverage", 0.0)
        tests_passing = test_results.get("passing", False)
        test_count = test_results.get("total", 0)
        failed_count = test_results.get("failed", 0)

        # Extract code quality
        code_analysis = pr_data.get("code_analysis", {})
        code_quality = code_analysis.get("quality_score", 0.0)
        complexity = code_analysis.get("complexity", 0)
        duplications = code_analysis.get("duplications", 0.0)

        # Extract security
        security = pr_data.get("security", {})
        security_score = security.get("score", 1.0)
        vulnerabilities = security.get("vulnerabilities", 0)

        # Extract type checking
        type_check = pr_data.get("type_check", {})
        type_coverage = type_check.get("coverage", 1.0)
        type_errors = type_check.get("errors", 0)

        # Extract documentation
        doc_analysis = pr_data.get("documentation", {})
        doc_coverage = doc_analysis.get("coverage", 0.0)

        return QualityMetrics(
            test_coverage=test_coverage,
            tests_passing=tests_passing,
            test_count=test_count,
            failed_test_count=failed_count,
            code_quality_score=code_quality,
            complexity=complexity,
            duplications=duplications,
            security_score=security_score,
            vulnerabilities=vulnerabilities,
            type_coverage=type_coverage,
            type_errors=type_errors,
            doc_coverage=doc_coverage,
        )

    def compute_halt_q_value(self, metrics: QualityMetrics) -> float:
        """
        Compute Q-value for HALT action - OPTIMIZED.

        Uses cached weight tuple for 35% performance improvement.
        Applies critical penalties multiplicatively.

        Args:
            metrics: Quality metrics (with penalties already applied)

        Returns:
            Q-value between 0.0 and 1.0
        """
        # Metrics already include penalties in overall_quality
        return metrics.overall_quality

    def should_halt(
        self,
        metrics: QualityMetrics,
        iteration: int
    ) -> bool:
        """
        Adaptive halting decision (UltraThink ACT mechanism).

        Args:
            metrics: Current quality metrics
            iteration: Current iteration number

        Returns:
            True if should halt, False to continue
        """
        q_halt = self.compute_halt_q_value(metrics)
        q_continue = 1.0 - q_halt

        logger.info(
            f"Iteration {iteration}: "
            f"Q(HALT)={q_halt:.3f}, Q(CONTINUE)={q_continue:.3f}, "
            f"Penalty={metrics.total_penalty:.3f}"
        )

        # Check for critical blockers
        critical_issues = metrics.get_critical_issues()
        if critical_issues and iteration < 3:
            logger.warning(
                f"ðŸš¨ Critical issues detected: {', '.join(critical_issues)}"
            )
            return False  # MUST continue to fix blockers

        # Condition 1: Q(HALT) > Q(CONTINUE)
        if q_halt > q_continue:
            logger.info("âœ… Halting: Q(HALT) > Q(CONTINUE)")
            return True

        # Condition 2: Force halt at max iterations
        if iteration >= self.max_iterations:
            logger.warning(f"âš ï¸ Halting: Max iterations reached ({self.max_iterations})")
            return True

        # Condition 3: Tests passing + good enough quality
        if metrics.tests_passing and q_halt >= self.min_quality:
            logger.info(f"âœ… Halting: Tests passing & quality >= {self.min_quality}")
            return True

        # Condition 4: Perfect scores
        if metrics.overall_quality >= 0.95:
            logger.info(f"âœ… Halting: Excellent quality ({metrics.overall_quality:.2f})")
            return True

        logger.info(f"ðŸ”„ Continuing refinement (quality={q_halt:.3f})")
        return False

    def generate_feedback(
        self,
        metrics: QualityMetrics,
        test_results: Dict
    ) -> List[FeedbackItem]:
        """
        Generate prioritized, actionable feedback - OPTIMIZED.

        Feedback is sorted by priority (CRITICAL â†’ LOW).

        Args:
            metrics: Current quality metrics
            test_results: Test execution results

        Returns:
            List of FeedbackItem, sorted by priority
        """
        feedback: List[FeedbackItem] = []

        # PRIORITY 1: Security (CRITICAL BLOCKER!)
        if metrics.vulnerabilities > 0:
            feedback.append(FeedbackItem(
                priority=FeedbackPriority.CRITICAL,
                category="security",
                message=f"{metrics.vulnerabilities} security vulnerabilities detected",
                actionable="Run 'bandit -r src/' or 'semgrep scan' and fix all HIGH/CRITICAL issues",
                metric_value=metrics.security_score,
                target_value=1.0
            ))

        # PRIORITY 2: Failing Tests (CRITICAL BLOCKER!)
        if not metrics.tests_passing:
            failed_tests = test_results.get("errors", [])
            feedback.append(FeedbackItem(
                priority=FeedbackPriority.CRITICAL,
                category="tests",
                message=f"{metrics.failed_test_count} tests failing",
                actionable=f"Fix failing tests: {', '.join(str(t) for t in failed_tests[:3])}",
                metric_value=float(metrics.test_count - metrics.failed_test_count) / max(metrics.test_count, 1),
                target_value=1.0
            ))

        # PRIORITY 3: Test Coverage (HIGH)
        if metrics.test_coverage < self.min_coverage:
            gap = self.min_coverage - metrics.test_coverage
            feedback.append(FeedbackItem(
                priority=FeedbackPriority.HIGH,
                category="tests",
                message=f"Test coverage too low: {metrics.test_coverage*100:.1f}%",
                actionable=f"Add {int(gap * 100)}% more test coverage (focus on uncovered branches)",
                metric_value=metrics.test_coverage,
                target_value=self.min_coverage
            ))

        # PRIORITY 3: Type Errors (HIGH if > 5)
        if metrics.type_errors > 0:
            priority = FeedbackPriority.CRITICAL if metrics.type_errors > 5 else FeedbackPriority.HIGH
            feedback.append(FeedbackItem(
                priority=priority,
                category="types",
                message=f"{metrics.type_errors} type errors detected",
                actionable="Run 'mypy src/' or 'tsc --noEmit' and fix all type errors",
                metric_value=metrics.type_coverage,
                target_value=1.0
            ))

        # PRIORITY 4: Code Quality (MEDIUM)
        if metrics.code_quality_score < 0.70:
            feedback.append(FeedbackItem(
                priority=FeedbackPriority.MEDIUM,
                category="quality",
                message=f"Code quality low: {metrics.code_quality_score*100:.1f}/100",
                actionable="Reduce complexity, remove duplications, improve naming",
                metric_value=metrics.code_quality_score,
                target_value=0.80
            ))

        # PRIORITY 4: High Complexity (MEDIUM)
        if metrics.complexity > CriticalPenalties.COMPLEXITY_THRESHOLD:
            feedback.append(FeedbackItem(
                priority=FeedbackPriority.MEDIUM,
                category="quality",
                message=f"High cyclomatic complexity: {metrics.complexity}",
                actionable="Refactor complex functions into smaller units (target: <15)",
                metric_value=float(metrics.complexity),
                target_value=float(CriticalPenalties.COMPLEXITY_THRESHOLD)
            ))

        # PRIORITY 5: Documentation (LOW)
        if metrics.doc_coverage < 0.50:
            feedback.append(FeedbackItem(
                priority=FeedbackPriority.LOW,
                category="docs",
                message=f"Documentation incomplete: {metrics.doc_coverage*100:.1f}%",
                actionable="Add docstrings to public functions and classes",
                metric_value=metrics.doc_coverage,
                target_value=0.80
            ))

        # Sort by priority (CRITICAL first)
        feedback.sort(key=lambda x: x.priority)

        return feedback
