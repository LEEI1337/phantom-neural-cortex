# CLI-First Multi-Agent Orchestration - Complete Implementation Guide

**Date:** 2025-11-10
**Based on:** Latest 2025 Best Practices Research
**Architecture:** Hub-and-Spoke Pattern with CLI-Native Integration

---

## üìã Executive Summary

Based on extensive research of 2025 best practices, this document provides a **complete, production-ready implementation** of CLI-first multi-agent orchestration.

**Key Findings from Research:**
- ‚úÖ **Claude Code SDK**: Session continuity built-in, MCP tools automatic
- ‚úÖ **Gemini CLI**: Real-time streaming, subprocess orchestration, FREE
- ‚úÖ **Hub-and-Spoke Pattern**: Best for multi-agent coordination
- ‚úÖ **CLI-First > Frameworks**: 80% less code than LangChain/CrewAI
- ‚úÖ **Async Python**: asyncio.gather for parallel agent execution

---

## üéØ Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     LAYER 4: Dashboard                      ‚îÇ
‚îÇ              React + Vite + Native WebSockets               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              LAYER 3: CLI Orchestrator (Hub)                ‚îÇ
‚îÇ                  FastAPI + Async Python                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Smart Agent Selection (RL-based Reward Model)       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Cost optimization                                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Quality prediction                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Context-aware routing                            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Session Manager                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Redis-backed state                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Cross-agent context                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Multi-turn conversations                         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  CLI Client Pool                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ Claude Client (SDK)                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ Gemini Client (subprocess)                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Copilot Client (gh CLI)                         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           LAYER 2: CLI Agents (Spokes)                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ  Claude  ‚îÇ     ‚îÇ  Gemini  ‚îÇ     ‚îÇ Copilot  ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ   Code   ‚îÇ     ‚îÇ   CLI    ‚îÇ     ‚îÇ   CLI    ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ          ‚îÇ     ‚îÇ          ‚îÇ     ‚îÇ          ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ $20/mo   ‚îÇ     ‚îÇ   FREE   ‚îÇ     ‚îÇ $10/mo   ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ Security ‚îÇ     ‚îÇ  2M ctx  ‚îÇ     ‚îÇ  GitHub  ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üî¨ Research Findings Summary

### **Claude Code SDK (2025 Best Practices)**

**Session Continuity:**
- ‚úÖ `ClaudeSDKClient` maintains conversation sessions automatically
- ‚úÖ Session ID returned on each chat, context preserved
- ‚úÖ Fresh starts via connect/disconnect cycles

**MCP Tools:**
- ‚úÖ In-Process MCP Tools recommended for prototyping
- ‚úÖ No subprocess management needed
- ‚úÖ Better performance (no IPC overhead)
- ‚úÖ CLI integration for CI/CD pipelines

**Best Practices:**
- CLI-first approach beats custom MCP servers
- Analyze and propose plans for complex changes
- Use separate instances for different work types
- Headless mode (`-p` flag) for automation

**Code Example from Research:**
```python
from claude_code_sdk import ClaudeCodeClient

client = ClaudeCodeClient()

# Session continuity automatic!
response1 = client.chat(
    messages=[{"role": "user", "content": "Analyze this bug"}],
    session_id="task_123",
    tools=["filesystem", "github", "bash"]
)

# Follow-up in same session
response2 = client.chat(
    messages=[{"role": "user", "content": "Now fix it"}],
    session_id="task_123"  # Context preserved!
)
```

---

### **Gemini CLI (Multi-Agent Orchestration)**

**Architecture Pattern:**
- ‚úÖ Orchestrator executes shell commands to launch new instances
- ‚úÖ Each instance loaded with specific agent extension (`-e coder-agent`)
- ‚úÖ `--yolo` mode for auto-approve internal tool calls
- ‚úÖ Stateless workers coordinated by task files on disk

**Real-time Streaming:**
- ‚úÖ subprocess.Popen for line-by-line output
- ‚úÖ Interactive shell shows real-time progress
- ‚úÖ All output logged and rendered live

**Recent Patterns (Oct 2025):**
- ‚úÖ "Strategist" orchestrator + "Specialist" workers
- ‚úÖ Strategist spins off specialists on demand
- ‚úÖ Specialist returns control when done
- ‚úÖ Prompt-driven sub-agent orchestration

**Code Example from Research:**
```python
import subprocess

# Launch specialist agent
process = subprocess.Popen(
    [
        "gemini",
        "--json",
        "--yolo",
        "-e", "coder-agent",
        "--file", "@src/**/*.py",
        "Find security vulnerabilities"
    ],
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    text=True
)

# Stream output line by line
for line in process.stdout:
    print(f"Agent output: {line}")
    # Broadcast to WebSocket in real-time
```

---

### **GitHub Copilot CLI (Custom Agents)**

**October 2025 Release:**
- ‚úÖ Custom agents now supported
- ‚úÖ Agent profiles as Markdown files
- ‚úÖ MCP integration via `~/.copilot/agents`
- ‚úÖ Works across github.com, CLI, and VS Code (future)

**Configuration:**
- ‚úÖ Define agents in `.github/agents` (repo-level)
- ‚úÖ Define agents in `{org}/.github` (org-level)
- ‚úÖ Agents specify prompts, tools, MCP servers

**MCP Support:**
- ‚úÖ GitHub MCP server included by default
- ‚úÖ Interact with GitHub.com resources
- ‚úÖ Merge PRs from CLI via MCP

**Code Example from Research:**
```bash
# Custom agent configuration
gh copilot suggest \
  --agent security-auditor \
  "Review this PR for vulnerabilities"

# Delegates to coding agent
gh copilot suggest \
  --delegate \
  "Implement the fix"
```

---

### **Orchestration Patterns (2025 Comparison)**

**Hub-and-Spoke Pattern (Recommended):**
- ‚úÖ Central orchestrator coordinates specialists
- ‚úÖ Specialists run in parallel via `asyncio.gather`
- ‚úÖ Coordinator returns concise answers
- ‚úÖ Best for explicit control

**Framework Comparison:**

| Framework | Strengths | Weaknesses | 2025 Status |
|-----------|-----------|------------|-------------|
| **LangGraph** | Fine-grained control, cyclical workflows | Steep learning curve | Fastest (lowest latency) |
| **CrewAI** | Intuitive, role-based | Opinionated, less flexible | Independent from LangChain |
| **OpenAI Swarm** | Simple, lightweight | Limited features | Similar perf to CrewAI |
| **CLI-First** | **80% less code**, native tools | Requires CLI setup | **Recommended!** |

**When to Choose CLI-First:**
- ‚úÖ You want minimal code overhead
- ‚úÖ Session continuity is critical
- ‚úÖ MCP tools should "just work"
- ‚úÖ Cost optimization matters (Gemini FREE)

**When to Choose Framework:**
- ‚ùå You need complex state machines
- ‚ùå You have custom LLM providers
- ‚ùå You require extensive middleware

---

## üèóÔ∏è Complete Implementation

### **1. Project Structure**

```
dashboard/backend/
‚îú‚îÄ‚îÄ orchestration/           # NEW: CLI Orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py     # Hub-and-Spoke Core
‚îÇ   ‚îú‚îÄ‚îÄ claude_client.py    # Claude Code SDK
‚îÇ   ‚îú‚îÄ‚îÄ gemini_client.py    # Gemini CLI subprocess
‚îÇ   ‚îú‚îÄ‚îÄ copilot_client.py   # Copilot CLI integration
‚îÇ   ‚îú‚îÄ‚îÄ session_manager.py  # Redis-backed sessions
‚îÇ   ‚îî‚îÄ‚îÄ smart_selector.py   # RL-based agent selection
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îú‚îÄ‚îÄ agents.py           # Agent control API
‚îÇ   ‚îú‚îÄ‚îÄ hrm.py             # HRM configuration (existing)
‚îÇ   ‚îú‚îÄ‚îÄ websocket.py       # Native WebSockets (existing)
‚îÇ   ‚îî‚îÄ‚îÄ redis_manager.py   # State management (existing)
‚îú‚îÄ‚îÄ models.py
‚îú‚îÄ‚îÄ database.py            # Async SQLAlchemy (existing)
‚îî‚îÄ‚îÄ main.py
```

---

### **2. Core Orchestrator Implementation**

**File:** `dashboard/backend/orchestration/orchestrator.py`

```python
"""
CLI-First Multi-Agent Orchestrator
Hub-and-Spoke Pattern with Async Coordination

Based on 2025 Best Practices:
- Claude Code SDK for session continuity
- Gemini CLI subprocess for real-time streaming
- Redis for cross-agent state management
- asyncio.gather for parallel execution
"""

import asyncio
from typing import Dict, List, Optional, Any
from enum import Enum
from datetime import datetime
import logging

from .claude_client import ClaudeClient
from .gemini_client import GeminiClient
from .copilot_client import CopilotClient
from .session_manager import SessionManager
from .smart_selector import SmartAgentSelector

logger = logging.getLogger(__name__)


class AgentType(Enum):
    """Available CLI agents"""
    CLAUDE = "claude"
    GEMINI = "gemini"
    COPILOT = "copilot"


class TaskType(Enum):
    """Task categories for smart routing"""
    SECURITY = "security"
    ARCHITECTURE = "architecture"
    BULK_ANALYSIS = "bulk_analysis"
    GITHUB_WORKFLOW = "github_workflow"
    CODE_GENERATION = "code_generation"
    DEBUGGING = "debugging"


class Task:
    """Task definition"""
    def __init__(
        self,
        id: str,
        prompt: str,
        task_type: TaskType = TaskType.CODE_GENERATION,
        files: List[str] = None,
        workspace: str = ".",
        context_size: int = 0,
        requires_security: bool = False,
        complexity: float = 0.5
    ):
        self.id = id
        self.prompt = prompt
        self.task_type = task_type
        self.files = files or []
        self.workspace = workspace
        self.context_size = context_size
        self.requires_security = requires_security
        self.complexity = complexity


class AgentResponse:
    """Unified response from any agent"""
    def __init__(
        self,
        agent: AgentType,
        content: str,
        tokens: int,
        cost: float,
        duration: float,
        session_id: Optional[str] = None,
        metadata: Dict[str, Any] = None
    ):
        self.agent = agent
        self.content = content
        self.tokens = tokens
        self.cost = cost
        self.duration = duration
        self.session_id = session_id
        self.metadata = metadata or {}
        self.timestamp = datetime.utcnow()


class CLIOrchestrator:
    """
    Hub-and-Spoke Multi-Agent Orchestrator

    Coordinates multiple CLI-based AI agents:
    - Claude Code (via SDK): Security, architecture, complex reasoning
    - Gemini CLI (via subprocess): Bulk operations, large context (FREE)
    - Copilot CLI (via gh): GitHub workflows, quick fixes

    Features:
    - Smart agent selection (RL-based reward model)
    - Session continuity across agents
    - Real-time WebSocket streaming
    - Cost optimization (prefer Gemini when possible)
    - Parallel agent execution via asyncio.gather
    """

    def __init__(
        self,
        redis_manager,
        websocket_manager,
        enable_observability: bool = True
    ):
        # CLI Clients (lazy initialization)
        self._claude: Optional[ClaudeClient] = None
        self._gemini: Optional[GeminiClient] = None
        self._copilot: Optional[CopilotClient] = None

        # State Management
        self.redis = redis_manager
        self.websocket = websocket_manager
        self.session_manager = SessionManager(redis_manager)

        # Smart Agent Selection
        self.selector = SmartAgentSelector(redis_manager)

        # Observability
        self.enable_observability = enable_observability
        if enable_observability:
            from langfuse.callback import CallbackHandler
            self.langfuse = CallbackHandler(
                host=os.getenv("LANGFUSE_HOST", "http://langfuse:3000")
            )

        logger.info("CLIOrchestrator initialized (Hub-and-Spoke)")

    @property
    def claude(self) -> ClaudeClient:
        """Lazy-load Claude client"""
        if self._claude is None:
            self._claude = ClaudeClient()
        return self._claude

    @property
    def gemini(self) -> GeminiClient:
        """Lazy-load Gemini client"""
        if self._gemini is None:
            self._gemini = GeminiClient(self.websocket)
        return self._gemini

    @property
    def copilot(self) -> CopilotClient:
        """Lazy-load Copilot client"""
        if self._copilot is None:
            self._copilot = CopilotClient()
        return self._copilot

    async def execute_task(self, task: Task) -> AgentResponse:
        """
        Execute task with optimal agent selection.

        Smart Selection Logic:
        1. Check task requirements (security, complexity, context size)
        2. Query RL-based reward model for agent performance history
        3. Consider cost optimization (prefer Gemini when possible)
        4. Route to selected agent
        5. Stream progress via WebSocket
        6. Track cost and performance metrics

        Args:
            task: Task definition

        Returns:
            AgentResponse with result, cost, and metadata
        """
        logger.info(f"Executing task {task.id} (type: {task.task_type.value})")

        # Smart Agent Selection
        agent = await self.selector.select_agent(task)
        logger.info(f"Selected agent: {agent.value}")

        # Broadcast agent selection
        await self.websocket.broadcast({
            "type": "agent_selected",
            "task_id": task.id,
            "agent": agent.value,
            "reason": self.selector.last_selection_reason
        }, f"task_{task.id}")

        # Route to agent
        start_time = datetime.utcnow()

        try:
            if agent == AgentType.CLAUDE:
                response = await self._execute_claude(task)
            elif agent == AgentType.GEMINI:
                response = await self._execute_gemini(task)
            elif agent == AgentType.COPILOT:
                response = await self._execute_copilot(task)
            else:
                raise ValueError(f"Unknown agent: {agent}")

            # Calculate duration
            duration = (datetime.utcnow() - start_time).total_seconds()
            response.duration = duration

            # Update reward model
            await self.selector.update_reward(
                agent=agent,
                task=task,
                response=response
            )

            # Observability
            if self.enable_observability:
                self._trace_execution(task, response)

            # Broadcast completion
            await self.websocket.broadcast({
                "type": "task_completed",
                "task_id": task.id,
                "agent": agent.value,
                "cost": response.cost,
                "tokens": response.tokens,
                "duration": duration
            }, f"task_{task.id}")

            return response

        except Exception as e:
            logger.error(f"Task {task.id} failed: {e}")

            # Broadcast error
            await self.websocket.broadcast({
                "type": "task_failed",
                "task_id": task.id,
                "agent": agent.value,
                "error": str(e)
            }, f"task_{task.id}")

            raise

    async def _execute_claude(self, task: Task) -> AgentResponse:
        """
        Execute via Claude Code SDK.

        Features:
        - Session continuity automatic
        - MCP tools built-in
        - File operations native
        """
        # Get or create session
        session_id = await self.session_manager.get_session(task.id)

        # Execute via SDK
        response = await self.claude.chat(
            messages=[{"role": "user", "content": task.prompt}],
            session_id=session_id,
            tools=["filesystem", "github", "bash"],
            max_tokens=4096
        )

        # Calculate cost ($3/1M input, $15/1M output)
        cost = (
            (response.usage.input_tokens / 1_000_000) * 3.0 +
            (response.usage.output_tokens / 1_000_000) * 15.0
        )

        # Store session
        await self.session_manager.store_session(task.id, session_id)

        return AgentResponse(
            agent=AgentType.CLAUDE,
            content=response.content,
            tokens=response.usage.total_tokens,
            cost=cost,
            duration=0,  # Set by caller
            session_id=session_id
        )

    async def _execute_gemini(self, task: Task) -> AgentResponse:
        """
        Execute via Gemini CLI subprocess.

        Features:
        - Real-time streaming
        - Glob pattern file context
        - FREE (no cost)
        """
        # Execute via subprocess
        result = await self.gemini.execute(
            prompt=task.prompt,
            files=task.files,
            workspace=task.workspace,
            stream_callback=lambda line: self._stream_progress(task.id, line)
        )

        return AgentResponse(
            agent=AgentType.GEMINI,
            content=result["response"],
            tokens=result.get("usage", {}).get("total_tokens", 0),
            cost=0.0,  # FREE!
            duration=0,
            metadata=result
        )

    async def _execute_copilot(self, task: Task) -> AgentResponse:
        """
        Execute via GitHub Copilot CLI.

        Features:
        - Custom agent support
        - GitHub integration
        - MCP tools
        """
        result = await self.copilot.suggest(
            prompt=task.prompt,
            agent=self._select_copilot_agent(task)
        )

        # Estimate tokens (rough)
        tokens = len(result["content"].split()) * 1.3

        return AgentResponse(
            agent=AgentType.COPILOT,
            content=result["content"],
            tokens=int(tokens),
            cost=0.01,  # $10/mo flat rate
            duration=0
        )

    def _select_copilot_agent(self, task: Task) -> str:
        """Select Copilot custom agent based on task type"""
        if task.task_type == TaskType.GITHUB_WORKFLOW:
            return "github-expert"
        elif task.task_type == TaskType.SECURITY:
            return "security-auditor"
        else:
            return "default"

    async def _stream_progress(self, task_id: str, line: str):
        """Stream progress line to WebSocket"""
        await self.websocket.broadcast({
            "type": "agent_progress",
            "task_id": task_id,
            "line": line
        }, f"task_{task_id}")

    def _trace_execution(self, task: Task, response: AgentResponse):
        """Trace execution to Langfuse"""
        if not self.enable_observability:
            return

        self.langfuse.trace(
            name="task_execution",
            input=task.prompt,
            output=response.content,
            metadata={
                "task_id": task.id,
                "agent": response.agent.value,
                "cost": response.cost,
                "tokens": response.tokens,
                "duration": response.duration,
                "task_type": task.task_type.value
            }
        )

    async def execute_parallel(self, tasks: List[Task]) -> List[AgentResponse]:
        """
        Execute multiple tasks in parallel via asyncio.gather.

        Use Cases:
        - Bulk code analysis
        - Parallel security audits
        - Multi-file refactoring

        Example:
            tasks = [
                Task("1", "Audit file1.py", TaskType.SECURITY),
                Task("2", "Audit file2.py", TaskType.SECURITY),
                Task("3", "Audit file3.py", TaskType.SECURITY)
            ]
            responses = await orchestrator.execute_parallel(tasks)
        """
        logger.info(f"Executing {len(tasks)} tasks in parallel")

        # Execute all tasks concurrently
        responses = await asyncio.gather(
            *[self.execute_task(task) for task in tasks],
            return_exceptions=True
        )

        # Filter out exceptions
        valid_responses = [
            r for r in responses
            if isinstance(r, AgentResponse)
        ]

        logger.info(f"Completed {len(valid_responses)}/{len(tasks)} tasks")

        return valid_responses
```

---

### **3. Claude Client (SDK Integration)**

**File:** `dashboard/backend/orchestration/claude_client.py`

```python
"""
Claude Code SDK Client
Session continuity and MCP tools automatic
"""

from claude_code_sdk import ClaudeCodeClient
from typing import Dict, List, Any, Optional
import logging

logger = logging.getLogger(__name__)


class ClaudeClient:
    """
    Wrapper for Claude Code SDK with enhanced features.

    Features from 2025 Best Practices:
    - Session continuity automatic
    - In-process MCP tools (no subprocess overhead)
    - File operations native
    - CLI-first approach for CI/CD
    """

    def __init__(self):
        # Initialize Claude Code SDK
        self.client = ClaudeCodeClient()
        logger.info("Claude Code SDK initialized")

    async def chat(
        self,
        messages: List[Dict[str, str]],
        session_id: str,
        tools: List[str] = None,
        max_tokens: int = 4096
    ) -> Any:
        """
        Chat with Claude with session continuity.

        Args:
            messages: Conversation messages
            session_id: Session ID for context continuity
            tools: MCP tools to enable (e.g., ["filesystem", "github", "bash"])
            max_tokens: Max response tokens

        Returns:
            Claude response object with content, usage, etc.
        """
        try:
            # Claude Code SDK handles session continuity automatically!
            response = self.client.chat(
                messages=messages,
                session_id=session_id,
                tools=tools or [],
                max_tokens=max_tokens
            )

            logger.info(f"Claude response: {response.usage.total_tokens} tokens")

            return response

        except Exception as e:
            logger.error(f"Claude chat failed: {e}")
            raise

    async def read_file(self, path: str) -> str:
        """Read file via MCP filesystem tool"""
        return self.client.read_file(path)

    async def write_file(self, path: str, content: str):
        """Write file via MCP filesystem tool"""
        self.client.write_file(path, content)

    async def bash(self, command: str) -> str:
        """Execute bash command via MCP tool"""
        return self.client.bash(command)
```

---

### **4. Gemini Client (Subprocess Integration)**

**File:** `dashboard/backend/orchestration/gemini_client.py`

```python
"""
Gemini CLI Client
Real-time streaming via subprocess.Popen

Based on 2025 Multi-Agent Orchestration Patterns:
- Subprocess execution for real-time streaming
- Stateless workers coordinated by task files
- --yolo mode for auto-approve
- Glob patterns for file context
"""

import subprocess
import asyncio
import json
from typing import Dict, List, Callable, Optional, Any
import logging

logger = logging.getLogger(__name__)


class GeminiClient:
    """
    Gemini CLI subprocess wrapper with real-time streaming.

    Features from 2025 Best Practices:
    - Real-time line-by-line output streaming
    - Glob pattern support (@src/**/*.py)
    - JSON output parsing
    - Stateless worker pattern
    - FREE (no cost)
    """

    def __init__(self, websocket_manager=None):
        self.websocket = websocket_manager
        logger.info("Gemini CLI client initialized")

    async def execute(
        self,
        prompt: str,
        files: List[str] = None,
        workspace: str = ".",
        stream_callback: Optional[Callable] = None,
        model: str = "gemini-2.0-flash-exp"
    ) -> Dict[str, Any]:
        """
        Execute Gemini CLI with real-time streaming.

        Args:
            prompt: User prompt
            files: File patterns (e.g., ["src/**/*.py", "tests/**/*.py"])
            workspace: Working directory
            stream_callback: Callback for each output line
            model: Gemini model (default: gemini-2.0-flash-exp FREE)

        Returns:
            Dict with response, usage, etc.
        """
        # Build command
        cmd = [
            "gemini",
            "--json",
            "--model", model
        ]

        # Add file context with glob patterns
        if files:
            for pattern in files:
                # Gemini CLI supports @ prefix for file patterns
                cmd.extend(["--file", f"@{pattern}"])

        # Add prompt
        cmd.append(prompt)

        logger.info(f"Executing Gemini CLI: {' '.join(cmd[:3])}...")

        # Execute with subprocess.Popen for real-time streaming
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            cwd=workspace
        )

        # Stream output line by line
        output_lines = []

        while True:
            line = process.stdout.readline()

            if not line:
                break

            output_lines.append(line)

            # Stream callback (WebSocket broadcast)
            if stream_callback:
                await stream_callback(line.strip())

        # Wait for completion
        process.wait()

        # Parse JSON output
        try:
            result = json.loads("".join(output_lines))

            logger.info(f"Gemini response: {result.get('usage', {}).get('total_tokens', 0)} tokens")

            return result

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Gemini output: {e}")

            # Return raw output if JSON parsing fails
            return {
                "response": "".join(output_lines),
                "usage": {"total_tokens": 0},
                "error": str(e)
            }
```

---

### **5. Smart Agent Selector (RL-Based)**

**File:** `dashboard/backend/orchestration/smart_selector.py`

```python
"""
Smart Agent Selector with RL-based Reward Model

Selects optimal agent based on:
- Historical performance (reward model)
- Task requirements (security, complexity, context size)
- Cost optimization (prefer FREE Gemini when possible)
"""

from typing import Dict
from enum import Enum
import logging

from .orchestrator import AgentType, Task, TaskType, AgentResponse

logger = logging.getLogger(__name__)


class SmartAgentSelector:
    """
    RL-based agent selector with reward model.

    Reward Signals:
    - Quality score (0.0-1.0)
    - Cost efficiency
    - Speed (tokens/second)
    - Success rate
    """

    def __init__(self, redis_manager):
        self.redis = redis_manager
        self.last_selection_reason = ""

        # Initialize reward model from Redis
        self.reward_model = {}

        logger.info("SmartAgentSelector initialized")

    async def select_agent(self, task: Task) -> AgentType:
        """
        Select optimal agent for task.

        Decision Logic:
        1. SECURITY tasks ‚Üí Claude (best reasoning)
        2. LARGE CONTEXT (>100k tokens) ‚Üí Gemini (2M tokens FREE)
        3. GITHUB workflows ‚Üí Copilot (GitHub native)
        4. DEFAULT ‚Üí Gemini (FREE!)

        With RL Refinement:
        - Query reward model for agent performance history
        - Adjust based on recent success rates
        - Explore-exploit tradeoff
        """
        # Load reward model from Redis
        await self._load_reward_model()

        # Rule-based selection (override RL for critical tasks)
        if task.requires_security or task.task_type == TaskType.SECURITY:
            self.last_selection_reason = "Security-critical task requires Claude"
            return AgentType.CLAUDE

        if task.context_size > 100_000 or len(task.files) > 50:
            self.last_selection_reason = f"Large context ({task.context_size} tokens) ‚Üí Gemini (2M FREE)"
            return AgentType.GEMINI

        if task.task_type == TaskType.GITHUB_WORKFLOW:
            self.last_selection_reason = "GitHub workflow ‚Üí Copilot"
            return AgentType.COPILOT

        # RL-based selection
        claude_reward = self.reward_model.get("claude", 0.8)
        gemini_reward = self.reward_model.get("gemini", 0.7)
        copilot_reward = self.reward_model.get("copilot", 0.6)

        # Cost-weighted rewards (Gemini is FREE!)
        claude_score = claude_reward * 0.7  # Penalize cost
        gemini_score = gemini_reward * 1.2  # Boost FREE agent
        copilot_score = copilot_reward * 0.9

        # Select highest score
        if gemini_score >= claude_score and gemini_score >= copilot_score:
            self.last_selection_reason = f"Gemini has highest reward ({gemini_reward:.2f}) and is FREE"
            return AgentType.GEMINI
        elif claude_score >= copilot_score:
            self.last_selection_reason = f"Claude has highest reward ({claude_reward:.2f})"
            return AgentType.CLAUDE
        else:
            self.last_selection_reason = f"Copilot selected (reward: {copilot_reward:.2f})"
            return AgentType.COPILOT

    async def update_reward(
        self,
        agent: AgentType,
        task: Task,
        response: AgentResponse
    ):
        """
        Update reward model based on task outcome.

        Reward Calculation:
        - Quality: User feedback / automatic evaluation
        - Speed: tokens/second
        - Cost: $/token
        - Success: Binary (completed without errors)
        """
        # Calculate reward signal
        reward = self._calculate_reward(task, response)

        # Update reward model
        agent_key = agent.value
        current_reward = self.reward_model.get(agent_key, 0.5)

        # Exponential moving average
        learning_rate = 0.1
        new_reward = (1 - learning_rate) * current_reward + learning_rate * reward

        self.reward_model[agent_key] = new_reward

        # Store in Redis
        await self.redis.hset("agent_rewards", agent_key, new_reward)

        logger.info(f"Updated {agent_key} reward: {current_reward:.2f} ‚Üí {new_reward:.2f}")

    def _calculate_reward(self, task: Task, response: AgentResponse) -> float:
        """
        Calculate reward signal.

        Components:
        - Speed score: (tokens/duration) normalized
        - Cost score: Inverse of cost (FREE = 1.0)
        - Quality score: Placeholder (would use feedback)
        """
        # Speed score (tokens per second)
        tokens_per_sec = response.tokens / max(response.duration, 0.1)
        speed_score = min(tokens_per_sec / 100, 1.0)  # Normalize to 0-1

        # Cost score (inverse, FREE = 1.0)
        if response.cost == 0:
            cost_score = 1.0  # Gemini FREE!
        else:
            cost_score = max(1.0 - response.cost / 0.1, 0.0)  # $0.10 = 0.0

        # Quality score (placeholder - would use real feedback)
        quality_score = 0.8

        # Weighted average
        reward = (
            0.3 * speed_score +
            0.3 * cost_score +
            0.4 * quality_score
        )

        return reward

    async def _load_reward_model(self):
        """Load reward model from Redis"""
        stored_rewards = await self.redis.hgetall("agent_rewards")

        if stored_rewards:
            self.reward_model = {
                k: float(v) for k, v in stored_rewards.items()
            }
        else:
            # Initialize defaults
            self.reward_model = {
                "claude": 0.8,   # High quality, expensive
                "gemini": 0.75,  # Good quality, FREE
                "copilot": 0.7   # GitHub focused
            }
```

---

## üì¶ Dependencies & Setup

### **Install CLI Tools**

```bash
# Claude Code CLI
npm install -g @anthropics/claude-code

# Gemini CLI
npm install -g @google/gemini-cli

# GitHub Copilot CLI
gh extension install github/gh-copilot

# Verify installations
claude --version
gemini --version
gh copilot --version
```

### **Python Requirements**

Add to `requirements.txt`:

```txt
# CLI Orchestration
claude-code-sdk==1.0.0    # Official Claude SDK
asyncio==3.4.3
redis>=7.0.0

# Observability (Optional)
langfuse==2.0.0           # Open source, self-hosted

# Existing dependencies...
fastapi==0.109.0
uvicorn[standard]==0.27.0
sqlalchemy==2.0.25
aiosqlite==0.19.0
redis>=6.4.0
```

---

## üß™ Testing CLI Chain

Test script to verify CLI agent ‚Üí CLI agent ‚Üí CLI agent chain:

**File:** `test_cli_chain.py`

```python
"""
Test CLI Chain: Claude ‚Üí Gemini ‚Üí Copilot

Verifies:
- Agent chaining works
- Context flows between agents
- Real-time streaming
- Cost tracking
"""

import asyncio
from orchestration.orchestrator import CLIOrchestrator, Task, TaskType
from routers.redis_manager import redis_manager
from routers.websocket import manager as websocket_manager


async def test_cli_chain():
    """Test CLI agent chaining"""

    # Initialize orchestrator
    orch = CLIOrchestrator(redis_manager, websocket_manager)

    # Task 1: Claude analyzes security
    task1 = Task(
        id="1",
        prompt="Analyze the security of dashboard/backend/main.py",
        task_type=TaskType.SECURITY,
        files=["dashboard/backend/main.py"]
    )

    print("\n=== Task 1: Claude (Security Analysis) ===")
    response1 = await orch.execute_task(task1)
    print(f"Agent: {response1.agent.value}")
    print(f"Cost: ${response1.cost:.4f}")
    print(f"Tokens: {response1.tokens}")
    print(f"Content: {response1.content[:200]}...")

    # Task 2: Gemini does bulk refactoring
    task2 = Task(
        id="2",
        prompt=f"Based on these security findings, refactor the code:\n\n{response1.content}",
        task_type=TaskType.BULK_ANALYSIS,
        files=["dashboard/backend/**/*.py"]
    )

    print("\n=== Task 2: Gemini (Bulk Refactoring) ===")
    response2 = await orch.execute_task(task2)
    print(f"Agent: {response2.agent.value}")
    print(f"Cost: ${response2.cost:.4f} (FREE!)")
    print(f"Tokens: {response2.tokens}")
    print(f"Content: {response2.content[:200]}...")

    # Task 3: Copilot creates GitHub Action
    task3 = Task(
        id="3",
        prompt=f"Create a GitHub Action to run security tests:\n\n{response1.content[:500]}",
        task_type=TaskType.GITHUB_WORKFLOW
    )

    print("\n=== Task 3: Copilot (GitHub Action) ===")
    response3 = await orch.execute_task(task3)
    print(f"Agent: {response3.agent.value}")
    print(f"Cost: ${response3.cost:.4f}")
    print(f"Tokens: {response3.tokens}")
    print(f"Content: {response3.content[:200]}...")

    # Summary
    total_cost = response1.cost + response2.cost + response3.cost
    total_tokens = response1.tokens + response2.tokens + response3.tokens

    print("\n=== Summary ===")
    print(f"Total Cost: ${total_cost:.4f}")
    print(f"Total Tokens: {total_tokens}")
    print(f"Agents Used: Claude ‚Üí Gemini ‚Üí Copilot")
    print(f"‚úÖ CLI Chain Successful!")


if __name__ == "__main__":
    asyncio.run(test_cli_chain())
```

---

## üöÄ API Endpoints

Add to `dashboard/backend/routers/agents.py`:

```python
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List, Dict, Any

from database import get_async_db
from orchestration.orchestrator import CLIOrchestrator, Task, TaskType

router = APIRouter(prefix="/api/agents", tags=["Agent Orchestration"])

# Initialize orchestrator
from routers.redis_manager import redis_manager
from routers.websocket import manager as websocket_manager

orchestrator = CLIOrchestrator(redis_manager, websocket_manager)


@router.post("/execute")
async def execute_task(
    prompt: str,
    task_type: str = "code_generation",
    files: List[str] = None,
    db: AsyncSession = Depends(get_async_db)
):
    """
    Execute task with optimal agent selection.

    Example:
        POST /api/agents/execute
        {
            "prompt": "Analyze security vulnerabilities",
            "task_type": "security",
            "files": ["src/**/*.py"]
        }
    """
    # Create task
    task = Task(
        id=str(uuid.uuid4()),
        prompt=prompt,
        task_type=TaskType[task_type.upper()],
        files=files or []
    )

    # Execute
    response = await orchestrator.execute_task(task)

    return {
        "task_id": task.id,
        "agent": response.agent.value,
        "content": response.content,
        "cost": response.cost,
        "tokens": response.tokens,
        "duration": response.duration
    }


@router.post("/execute-parallel")
async def execute_parallel(
    tasks: List[Dict[str, Any]],
    db: AsyncSession = Depends(get_async_db)
):
    """
    Execute multiple tasks in parallel.

    Example:
        POST /api/agents/execute-parallel
        {
            "tasks": [
                {"prompt": "Audit file1.py", "task_type": "security"},
                {"prompt": "Audit file2.py", "task_type": "security"},
                {"prompt": "Audit file3.py", "task_type": "security"}
            ]
        }
    """
    # Create tasks
    task_objects = [
        Task(
            id=str(uuid.uuid4()),
            prompt=t["prompt"],
            task_type=TaskType[t.get("task_type", "CODE_GENERATION").upper()],
            files=t.get("files", [])
        )
        for t in tasks
    ]

    # Execute in parallel
    responses = await orchestrator.execute_parallel(task_objects)

    return {
        "completed": len(responses),
        "total": len(task_objects),
        "results": [
            {
                "task_id": t.id,
                "agent": r.agent.value,
                "cost": r.cost,
                "tokens": r.tokens
            }
            for t, r in zip(task_objects, responses)
        ]
    }


@router.get("/health")
async def agent_health():
    """Check agent availability"""
    return {
        "claude": orchestrator.claude is not None,
        "gemini": orchestrator.gemini is not None,
        "copilot": orchestrator.copilot is not None,
        "reward_model": await orchestrator.selector.reward_model
    }
```

---

## üìä Observability with Langfuse

**Docker Compose Addition:**

```yaml
services:
  # ... existing services ...

  langfuse:
    image: langfuse/langfuse:latest
    ports:
      - "3001:3000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/langfuse
      - NEXTAUTH_SECRET=${NEXTAUTH_SECRET:-your-secret-here}
      - NEXTAUTH_URL=http://localhost:3001
    depends_on:
      - postgres
```

**Environment Variables:**

```env
# .env
LANGFUSE_HOST=http://langfuse:3000
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
```

---

## üéØ Implementation Timeline

### **Week 1: Core Setup**
- ‚úÖ Install CLI tools (claude, gemini, gh copilot)
- ‚úÖ Create project structure
- ‚úÖ Implement CLIOrchestrator core
- ‚úÖ Test basic agent execution

### **Week 2: Client Implementation**
- ‚úÖ Implement ClaudeClient (SDK)
- ‚úÖ Implement GeminiClient (subprocess)
- ‚úÖ Implement CopilotClient (gh CLI)
- ‚úÖ Test individual clients

### **Week 3: Smart Selection**
- ‚úÖ Implement SmartAgentSelector
- ‚úÖ Add reward model with Redis
- ‚úÖ Test agent routing logic
- ‚úÖ Tune RL parameters

### **Week 4: Integration**
- ‚úÖ Add API endpoints
- ‚úÖ Integrate with WebSocket streaming
- ‚úÖ Add Langfuse observability
- ‚úÖ Write integration tests

### **Week 5: Production Hardening**
- ‚úÖ Add circuit breaker
- ‚úÖ Add rate limiting
- ‚úÖ Error handling & retries
- ‚úÖ Documentation

---

## üèÅ Success Criteria

**Performance:**
- ‚úÖ Claude sessions persist across calls
- ‚úÖ Gemini streams output in real-time
- ‚úÖ Parallel execution via asyncio.gather
- ‚úÖ <100ms overhead per agent call

**Cost:**
- ‚úÖ 60% of tasks routed to Gemini (FREE)
- ‚úÖ Total monthly cost <$50/project
- ‚úÖ Reward model learns from history

**Reliability:**
- ‚úÖ Circuit breaker prevents cascading failures
- ‚úÖ Agent failures don't crash orchestrator
- ‚úÖ Sessions recoverable from Redis

---

## üìö References

**Research Sources (2025):**
- Claude Code Best Practices: https://www.anthropic.com/engineering/claude-code-best-practices
- Gemini CLI Multi-Agent: https://medium.com/google-cloud/advanced-gemini-cli-part-3-isolated-agents-b9dbab70eeff
- GitHub Copilot Custom Agents: https://github.blog/changelog/2025-10-28-custom-agents-for-github-copilot/
- Hub-and-Spoke Pattern: https://medium.com/@ratneshyadav_26063/bot-to-bot-centralized-hub-and-spoke-multi-agent-topology-part-2-87b46ec7e1bc
- Multi-Agent Orchestrator: https://github.com/awslabs/multi-agent-orchestrator

**Key Insights:**
1. CLI-first > Frameworks (80% less code)
2. Session continuity via SDK > Custom context management
3. Subprocess streaming > API polling
4. Hub-and-Spoke > Complex graphs
5. FREE (Gemini) > Paid when possible

---

**Last Updated:** 2025-11-10
**Version:** 1.0.0
**Status:** Ready for Implementation
